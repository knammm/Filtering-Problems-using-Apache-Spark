scala> import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> val data = sc.textFile("test1/FPT-2018-12-02.log")
data: org.apache.spark.rdd.RDD[String] = test1/FPT-2018-12-02.log MapPartitionsRDD[47] at textFile at <console>:36

scala> data.count()
res26: Long = 1896813

scala>

scala> def filterRecords(line: String): Boolean = {
     |   val fields = line.split(" ")
     |   val criteria = fields.length == 7 && fields(0).toDouble >= 0 &&
     |     fields(6).forall(Character.isDigit) && fields(6).toInt > 0 &&
     |     fields(2) != "-"
     |   criteria
     | }
filterRecords: (line: String)Boolean

scala>

scala> val filterData = data.filter(filterRecords)
filterData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[48] at filter at <console>:37

scala> filterData.count()
res27: Long = 1303227

scala>

scala> def classifyService(line: String): String = {
     |   val contentName = line.split(" ")(5)
     |   if (contentName.endsWith(".mpd") || contentName.endsWith(".m3u8")) {
     |     "HLS"
     |   } else if (contentName.endsWith(".dash") || contentName.endsWith(".ts")) {
     |     "MPEG-DASH"
     |   } else {
     |     "Web Service"
     |   }
     | }
classifyService: (line: String)String

scala>

scala> val filteredAndClassifiedData = filterData.map(line => (classifyService(line), 1))
filteredAndClassifiedData: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[49] at map at <console>:37

scala> val serviceGroupCounts = filteredAndClassifiedData.reduceByKey(_ + _)
serviceGroupCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[50] at reduceByKey at <console>:36

scala> serviceGroupCounts.collect().foreach { case (serviceGroup, count) =>
     |   println(s"$serviceGroup: $count records")
     | }
HLS: 462938 records
MPEG-DASH: 826313 records
Web Service: 13976 records

scala>

scala> def extractIP(line: String): String = {
     |   val fields = line.split(" ")(1)
     |   fields
     | }
extractIP: (line: String)String

scala>

scala> val uniqueIPs = filterData.map(extractIP).distinct()
uniqueIPs: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at distinct at <console>:37

scala> uniqueIPs.count()
res29: Long = 3952

scala>

scala> val ipInfoData = sc.textFile("test1/IPDict.csv")
ipInfoData: org.apache.spark.rdd.RDD[String] = test1/IPDict.csv MapPartitionsRDD[56] at textFile at <console>:36

scala> ipInfoData.count()
res30: Long = 4849

scala>

scala> val ipInfoMap = ipInfoData.map(line => {
     |   val fields = line.split(",")
     |   (fields(0), (fields(1), fields(2), fields(3)))
     | }).collectAsMap()
ipInfoMap: scala.collection.Map[String,(String, String, String)] = Map(14.163.247.204 -> (Vietnam,Dien Bien Phu,Vietnam Posts and Telecommunications Group), 58.187.162.9 -> (Vietnam,Hanoi,FPT Telecom Company), 14.170.198.211 -> (Vietnam,Thanh Hoa,Vietnam Posts and Telecommunications Group), 1.55.195.66 -> (Vietnam,Ho Chi Minh City,FPT Telecom Company), 171.247.242.166 -> (Vietnam,Tra Tan,Viettel Group), 14.235.189.145 -> (Vietnam,Hanoi,Vietnam Posts and Telecommunications Group), 123.18.105.227 -> (Vietnam,Vinh,Vietnam Posts and Telecommunications Group), 58.187.67.212 -> (Vietnam,Vinh Yen,FPT Telecom Company), 66.249.82.103 -> (Australia,Sydney,Google LLC), 42.119.51.226 -> (Vietnam,Da Nang,FPT Telecom Company), 135.23.231.184 -> (Canada,Toronto,TekSavvy Solut...

scala> val ipInfoBroadcast = sc.broadcast(ipInfoMap)
ipInfoBroadcast: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,(String, String, String)]] = Broadcast(45)

scala>

scala> def enrichLogRecord(line: String): (String, (String, String, String), String, Double, String, Long) = {
     |   val fields = line.split(" ")
     |   val ip = fields(1)
     |   val additionalInfo = ipInfoBroadcast.value.getOrElse(ip, ("Unknown", "Unknown", "Unknown"))
     |   val latency = fields(0).toDouble
     |   val city = additionalInfo._2
     |   val contentSize = fields(fields.length - 1).toLong
     |   (ip, additionalInfo, city, latency, fields(4), contentSize)
     | }
enrichLogRecord: (line: String)(String, (String, String, String), String, Double, String, Long)

scala>

scala> val enrichedLogs = filterData.map(enrichLogRecord)
enrichedLogs: org.apache.spark.rdd.RDD[(String, (String, String, String), String, Double, String, Long)] = MapPartitionsRDD[58] at map at <console>:37

scala>

scala> val uniqueISPs = enrichedLogs.map{case (_, (_, _, isp), _, _, _, _) => isp}.distinct().collect()
uniqueISPs: Array[String] = Array(PJSC Rostelecom, China Unicom, Chunghwa Telecom, Verizon Business, ChinaNet, Telia Company AB, Telenor Norge AS, CD-Telematika a.s., RCS & RDS SA, Taiwan Mobile Co. Ltd., SFR SA, M1 Limited, Vodafone Telekomunikasyon A.S., Vodafone Australia Pty Limited, Kazan Broad-band access pools, CIK Telecom INC, Taipei Taiwan, Saigon Postel Corporation, SWAN a.s., M247 Ltd, Robert Bosch GmbH, "Facebook, Telstra Internet, TOKAI Communications Corporation, Orange S.A., Vietnamobile Telecommunications Joint Stock Company, Vietnam Posts and Telecommunications Group, Deutsche Telekom AG, TELEFiA?A1/2NICA BRASIL S.A, CTM, Vodafone NRW GmbH, Kddi Corporation, Spark New Zealand Trading Ltd, The Cloud Networks Limited, Tele Columbus AG, "Telia Lie...

scala> println(s"Number of unique ISPs: ${uniqueISPs.length}")
Number of unique ISPs: 125

scala>

scala> val hcmRecords = enrichedLogs.filter { case (_, (_, city, _), _, _, _, _) => city == "Ho Chi Minh City" }
hcmRecords: org.apache.spark.rdd.RDD[(String, (String, String, String), String, Double, String, Long)] = MapPartitionsRDD[63] at filter at <console>:36

scala> println(s"Number of records from Ho Chi Minh City: ${hcmRecords.count()}")
Number of records from Ho Chi Minh City: 217212

scala>

scala> val hanoiTraffic = enrichedLogs.filter { case (_, (_, city, _), _, _, _, _) => city == "Hanoi" }
hanoiTraffic: org.apache.spark.rdd.RDD[(String, (String, String, String), String, Double, String, Long)] = MapPartitionsRDD[64] at filter at <console>:36

scala>   .map { case (_, _, _, _, _, contentSize) => contentSize }
res33: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[65] at map at <console>:37

scala>   .reduce(_ + _)
res34: Long = 204245300091

scala> println(s"Total traffic from Hanoi: ${hanoiTraffic}")
Total traffic from Hanoi: MapPartitionsRDD[64] at filter at <console>:36

scala>

scala> val latencies = enrichedLogs.map { case (_, _, _, latency, _, _) => latency }
latencies: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[66] at map at <console>:36

scala> val latenciesVector = latencies.map(latency => Vectors.dense(latency))
latenciesVector: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[67] at map at <console>:36

scala> val latencyStats: MultivariateStatisticalSummary = Statistics.colStats(latenciesVector)
latencyStats: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@1c7f62f1

scala>

scala> println(s"Mean Latency: ${latencyStats.mean(0)}")
Mean Latency: 0.15163189835692353

scala> println(s"Maximum Latency: ${latencyStats.max(0)}")
Maximum Latency: 199.658

scala> println(s"Minimum Latency: ${latencyStats.min(0)}")
Minimum Latency: 0.0

scala>

scala> def getLatency(line: Double): Double = line
getLatency: (line: Double)Double

scala>

scala> val sortedLate = latencies.sortBy(getLatency)
sortedLate: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[73] at sortBy at <console>:37

scala> val median = (sortedLate.count() + 1)/2 - 1
median: Long = 651613

scala> val medianValue = sortedLate.collect()(median.toInt)
medianValue: Double = 0.0

scala> val maximumValue = sortedLate.collect()(sortedLate.count().toInt - 1)
maximumValue: Double = 199.658

scala> val secondMax = sortedLate.collect()(sortedLate.count().toInt - 2)
secondMax: Double = 119.467

scala>

scala> println(s"Median Latency: $medianValue")
Median Latency: 0.0

scala> println(s"Maximum Latency: $maximumValue")
Maximum Latency: 199.658

scala> println(s"Second Maximum Latency: $secondMax")
Second Maximum Latency: 119.467
